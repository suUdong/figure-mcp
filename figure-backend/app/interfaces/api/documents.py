"""
Documents API Endpoints
ë¬¸ì„œ ì—…ë¡œë“œ ë° ê´€ë¦¬ API
"""
import logging
import uuid
import os
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
from fastapi import APIRouter, HTTPException, Depends, status, UploadFile, File, Form, Request
from fastapi.responses import FileResponse
import aiofiles

from app.domain.entities.schemas import (
    Document, UploadDocumentRequest, APIResponse, DocumentType,
    JobCreate, JobType, JobUpdate, JobStatus
)
from app.domain.entities.template_entities import (
    Template, TemplateType, TemplateFormat, TemplateCreateRequest
)
from app.infrastructure.dependencies.logging_dependencies import business_logger_dependency
from app.infrastructure.middleware.logging_middleware import BusinessEventLogger
from app.application.services.vector_store import vector_store_service
from app.application.services.job_service import job_service
from app.application.services.template_service import get_template_service, TemplateService
from app.config import settings

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents", tags=["Documents"])

# ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì •)
UPLOAD_DIRECTORY = "/app/data/uploads"
os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)

# ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
SUPPORTED_EXTENSIONS = {
    '.txt': 'text',
    '.md': 'text',
    '.pdf': 'pdf',
    '.doc': 'doc',
    '.docx': 'doc',
    '.html': 'website',
    '.htm': 'website'
}

# í”„ë¡ íŠ¸ì—”ë“œ í…œí”Œë¦¿ íƒ€ì… ë§¤í•‘
FRONTEND_TEMPLATE_TYPE_MAPPING = {
    'REQUIREMENTS': 'requirements',
    'IMPACT_ANALYSIS': 'impact-analysis',
    'API_DOCUMENTATION': 'api-documentation',
    'DEPLOYMENT_GUIDE': 'deployment-guide',
    'TEST_PLAN': 'test-plan',
    'TECHNICAL_SPECIFICATION': 'technical-specification',
    'USER_MANUAL': 'user-manual',
    'RELEASE_NOTES': 'release-notes',
    'CUSTOM': 'custom'
}

async def get_vector_store_service():
    """ë²¡í„° ìŠ¤í† ì–´ ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ì£¼ì…"""
    if not vector_store_service._collection:
        await vector_store_service.initialize()
    return vector_store_service


@router.post(
    "/upload-file",
    response_model=APIResponse,
    summary="íŒŒì¼ ì—…ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ) - ë°±ì˜¤í”¼ìŠ¤ í˜¸í™˜",
    description="ë°±ì˜¤í”¼ìŠ¤ì—ì„œ ë³´ë‚´ëŠ” í˜•ì‹ì— ë§ì¶° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤. í…œí”Œë¦¿ì¸ ê²½ìš° SQLiteì—ë„ ì €ì¥ë©ë‹ˆë‹¤."
)
async def upload_file(
    request: Request,
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  íŒŒì¼"),
    site_id: Optional[str] = Form(default=None, description="ê´€ë ¨ ì‚¬ì´íŠ¸ ID"),
    metadata: Optional[str] = Form(default="{}", description="ë©”íƒ€ë°ì´í„° JSON ë¬¸ìì—´ (í…œí”Œë¦¿ ì •ë³´ í¬í•¨)"),
    service: object = Depends(get_vector_store_service),
    template_service: TemplateService = Depends(get_template_service),
    business_logger: BusinessEventLogger = Depends(business_logger_dependency)
) -> APIResponse:
    """
    ë°±ì˜¤í”¼ìŠ¤ í˜¸í™˜ íŒŒì¼ ì—…ë¡œë“œ
    
    **í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë³´ë‚´ëŠ” ë°ì´í„° êµ¬ì¡°:**
    - **file**: ì—…ë¡œë“œí•  íŒŒì¼ (í•„ìˆ˜)
    - **site_id**: ê´€ë ¨ ì‚¬ì´íŠ¸ ID (í•„ìˆ˜)
    - **metadata**: JSON ë¬¸ìì—´ë¡œ ë‹¤ìŒ ì •ë³´ í¬í•¨:
      - description: íŒŒì¼ ì„¤ëª…
      - tags: íƒœê·¸ ë°°ì—´
      - template_type: í…œí”Œë¦¿ ìœ í˜• (REQUIREMENTS, IMPACT_ANALYSIS ë“± - í•„ìˆ˜)
      - template_version: í…œí”Œë¦¿ ë²„ì „ (ê¸°ë³¸ê°’: "1.0.0")
    """
    # ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    log_context = getattr(request.state, 'log_context', None)
    
    # ë””ë²„ê¹…ì„ ìœ„í•œ ìš”ì²­ ë°ì´í„° ë¡œê¹…
    logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ìš”ì²­ ë°›ìŒ")
    logger.info(f"  - íŒŒì¼ëª…: {file.filename if file else 'None'}")
    logger.info(f"  - site_id ì›ë³¸ê°’: '{site_id}'")
    logger.info(f"  - site_id íƒ€ì…: {type(site_id)}")
    logger.info(f"  - metadata ì›ë³¸ê°’: '{metadata}'")
    
    try:
        # site_id ê²€ì¦ (í•„ìˆ˜ í•­ëª©) - ë¹ˆ ë¬¸ìì—´ë„ ì²´í¬
        if not site_id or site_id == "" or site_id.strip() == "":
            logger.error(f"site_id ê²€ì¦ ì‹¤íŒ¨: '{site_id}'")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ì‚¬ì´íŠ¸ IDëŠ” í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤."
            )
        
        # íŒŒì¼ ê¸°ë³¸ ê²€ì¦
        if not file or not file.filename:
            logger.error(f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: file={file}, filename={file.filename if file else 'None'}")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="íŒŒì¼ëª…ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_ext = os.path.splitext(file.filename)[1].lower() 
        if file_ext not in SUPPORTED_EXTENSIONS:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(SUPPORTED_EXTENSIONS.keys())}"
            )
        
        # ë©”íƒ€ë°ì´í„° íŒŒì‹±
        try:
            # metadataê°€ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
            if not metadata or metadata == "":
                parsed_metadata = {}
            else:
                parsed_metadata = json.loads(metadata) if metadata != "{}" else {}
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}, metadata: {metadata}")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ë©”íƒ€ë°ì´í„° JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}"
            )
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í…œí”Œë¦¿ ì •ë³´ ì¶”ì¶œ
        template_type_frontend = parsed_metadata.get('template_type', '')
        template_version = parsed_metadata.get('template_version', '1.0.0')
        description = parsed_metadata.get('description', '')
        tags = parsed_metadata.get('tags', [])
        
        # í…œí”Œë¦¿ ì—¬ë¶€ íŒë‹¨
        is_template = bool(template_type_frontend)
        
        # template_type ê²€ì¦ (í•„ìˆ˜)
        if not template_type_frontend or template_type_frontend == "":
            logger.error(f"template_type ê²€ì¦ ì‹¤íŒ¨: '{template_type_frontend}'")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="í…œí”Œë¦¿ ìœ í˜•ì€ í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤."
            )
        

            
        # í”„ë¡ íŠ¸ì—”ë“œ í…œí”Œë¦¿ íƒ€ì…ì„ ë°±ì—”ë“œ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘
        template_type_backend = FRONTEND_TEMPLATE_TYPE_MAPPING.get(
            template_type_frontend, 
            template_type_frontend.lower().replace('_', '-') if template_type_frontend else ''
        )
        
        # í…œí”Œë¦¿ ì´ë¦„ ìƒì„± (íŒŒì¼ëª… ê¸°ë°˜)
        template_name = os.path.splitext(file.filename)[0]
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸ ë¡œê¹…: íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘
        if log_context:
            await business_logger.logging_service.log_business_event(
                log_context,
                "íŒŒì¼_ì—…ë¡œë“œ_ì‹œì‘",
                {
                    "filename": file.filename,
                    "content_type": file.content_type,
                    "site_id": site_id,
                    "is_template": is_template,
                    "template_type": template_type_backend,
                    "template_name": template_name
                }
            )

        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (10MB ì œí•œ)
        MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
        content = await file.read()
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                detail="íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤."
            )
        
        # ì‘ì—… ì¶”ì  ì‹œì‘
        job = job_service.create_job(JobCreate(
            type=JobType.DOCUMENT_UPLOAD,
            site_id=site_id,
            metadata={
                "filename": file.filename,
                "file_size": len(content),
                "file_type": file_ext
            }
        ))
        
        try:
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: íŒŒì¼ ì €ì¥ ì¤‘
            job_service.update_job(job.id, JobUpdate(
                status=JobStatus.PROCESSING,
                progress=20.0,
                message="íŒŒì¼ ì €ì¥ ì¤‘..."
            ))
            
            # íŒŒì¼ ì €ì¥
            file_id = str(uuid.uuid4())
            file_path = os.path.join(UPLOAD_DIRECTORY, f"{file_id}{file_ext}")
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(content)
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ë‚´ìš© ì¶”ì¶œ ì¤‘
            job_service.update_job(job.id, JobUpdate(
                progress=40.0,
                message="ë‚´ìš© ì¶”ì¶œ ì¤‘..."
            ))
            
            # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
            file_content = await extract_file_content(file_path, file_ext)
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ë¬¸ì„œ ìƒì„± ì¤‘
            job_service.update_job(job.id, JobUpdate(
                progress=60.0,
                message="ë¬¸ì„œ ìƒì„± ì¤‘..."
            ))
            
                # ChromaDB í˜¸í™˜ì„ ìœ„í•´ ë°°ì—´ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            processed_metadata = {}
            for key, value in parsed_metadata.items():
                if isinstance(value, list):
                    processed_metadata[key] = ", ".join(str(item) for item in value)
                elif isinstance(value, dict):
                    processed_metadata[key] = json.dumps(value)
                else:
                    processed_metadata[key] = value
            
            # Document ê°ì²´ ìƒì„± (ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨)
            document = Document(
                id=file_id,
                title=file.filename,
                content=file_content,
                doc_type=DocumentType(SUPPORTED_EXTENSIONS[file_ext]),
                site_id=site_id,
                metadata={
                    "file_path": file_path,
                    "file_size": len(content),
                    "original_filename": file.filename,
                    "template_type": template_type_backend,
                    "template_name": template_name,
                    "template_description": description,
                    "template_version": template_version,
                    "is_template": is_template,
                    "description": description,
                    "tags": ", ".join(tags) if isinstance(tags, list) else tags,
                    **processed_metadata
                },
                source_url=file_path,
                created_at=datetime.now()
            )
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ë²¡í„°í™” ì¤‘
            job_service.update_job(job.id, JobUpdate(
                progress=80.0,
                message="ë²¡í„°í™” ì²˜ë¦¬ ì¤‘..."
            ))
            
            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            doc_id = await service.add_document(document)
            
            # í…œí”Œë¦¿ì¸ ê²½ìš° SQLiteì—ë„ ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥)
            template_id = None
            if is_template and template_type_backend:
                try:
                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: í…œí”Œë¦¿ ì €ì¥ ì¤‘
                    job_service.update_job(job.id, JobUpdate(
                        progress=90.0,
                        message="í…œí”Œë¦¿ ì €ì¥ ì¤‘..."
                    ))
                    
                    # ê¸°ì¡´ í…œí”Œë¦¿ ë²„ì „ í™•ì¸ ë° ìë™ ì¦ê°€
                    base_name = template_name
                    existing_versions = []
                    
                    try:
                        # ë°±ì—”ë“œ í…œí”Œë¦¿ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                        try:
                            template_type_enum = TemplateType(template_type_backend)
                        except ValueError:
                            # ë§¤í•‘ë˜ì§€ ì•Šì€ íƒ€ì…ì€ CUSTOMìœ¼ë¡œ ì²˜ë¦¬
                            template_type_enum = TemplateType.CUSTOM
                        
                        # ê¸°ì¡´ í…œí”Œë¦¿ë“¤ ì¡°íšŒ
                        existing_templates = await template_service.get_templates_by_filters(
                            template_type=template_type_enum,
                            site_id=site_id
                        )
                        
                        # ê°™ì€ ê¸°ë³¸ ì´ë¦„ì„ ê°€ì§„ í…œí”Œë¦¿ë“¤ì˜ ë²„ì „ ìˆ˜ì§‘
                        for template in existing_templates:
                            if template.name.startswith(base_name):
                                existing_versions.append(template.version)
                                
                    except Exception as e:
                        logger.warning(f"ê¸°ì¡´ í…œí”Œë¦¿ ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {e}")
                    
                    # ë²„ì „ ìë™ ì¦ê°€ ë¡œì§
                    final_version = template_version
                    if existing_versions:
                        # ê¸°ì¡´ ë²„ì „ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë²„ì „ ìƒì„±
                        try:
                            # ì‹œë§¨í‹± ë²„ì „ íŒŒì‹± ë° ì¦ê°€
                            latest_version = max(existing_versions, key=lambda v: [int(x) for x in v.split('.')])
                            version_parts = latest_version.split('.')
                            patch_version = int(version_parts[2]) + 1 if len(version_parts) >= 3 else 1
                            final_version = f"{version_parts[0]}.{version_parts[1]}.{patch_version}"
                            
                            logger.info(f"ìë™ ë²„ì „ ì¦ê°€: {latest_version} -> {final_version}")
                        except Exception as e:
                            logger.warning(f"ë²„ì „ ìë™ ì¦ê°€ ì‹¤íŒ¨, ê¸°ë³¸ ë²„ì „ ì‚¬ìš©: {e}")
                    
                    # í…œí”Œë¦¿ ìƒì„± ìš”ì²­ êµ¬ì„±
                    template_request = TemplateCreateRequest(
                        name=f"{base_name}_v{final_version}",
                        description=description or f"{file.filename}ì—ì„œ ì¶”ì¶œí•œ í…œí”Œë¦¿ (v{final_version})",
                        template_type=template_type_enum,
                        format=TemplateFormat.MARKDOWN if file_ext in ['.md'] else TemplateFormat.TEXT,
                        site_id=site_id,
                        content=file_content,
                        variables={},
                        tags=tags + ["uploaded", "hybrid", f"v{final_version}"] if isinstance(tags, list) else ["uploaded", "hybrid", f"v{final_version}"],
                        is_default=False,
                        version=final_version
                    )
                    
                    # í…œí”Œë¦¿ ì €ì¥
                    template = await template_service.create_template(
                        request=template_request,
                        created_by="hybrid_upload",
                        file_content=content,
                        file_name=file.filename,
                        content_type=file.content_type
                    )
                    template_id = template.id
                    
                    logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì™„ë£Œ - ë¬¸ì„œ: {doc_id}, í…œí”Œë¦¿: {template_id}")
                    
                except Exception as template_error:
                    logger.warning(f"í…œí”Œë¦¿ ì €ì¥ ì‹¤íŒ¨ (ë²¡í„° ì €ì¥ì€ ì„±ê³µ): {template_error}")
                    # í…œí”Œë¦¿ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë²¡í„° ì €ì¥ì€ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ
            job_service.update_job(job.id, JobUpdate(
                status=JobStatus.COMPLETED,
                progress=100.0,
                message="íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ" + (" (í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥)" if is_template else ""),
                metadata={
                    "document_id": doc_id,
                    "template_id": template_id
                }
            ))
            
            logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ: {file.filename} -> {doc_id}" + 
                       (f", í…œí”Œë¦¿: {template_id}" if template_id else ""))
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„± (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
            response_data = {
                "document_id": doc_id,
                "job_id": job.id,
                "filename": file.filename,
                "file_size": len(content),
                "doc_type": SUPPORTED_EXTENSIONS[file_ext],
                "created_at": document.created_at.isoformat(),
                "is_hybrid": is_template,
                "is_template": is_template,
                "template_type": template_type_backend if is_template else None,
                "template_name": template_name if is_template else None,
                "template_version": final_version if is_template and 'final_version' in locals() else template_version,
                "site_id": site_id,
                "description": description,
                "tags": tags
            }
            
            if template_id:
                response_data["template_id"] = template_id
            
            return APIResponse(
                success=True,
                message="íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ" + (" (í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì™„ë£Œ)" if is_template else ""),
                data=response_data
            )
            
        except Exception as e:
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹¤íŒ¨
            job_service.update_job(job.id, JobUpdate(
                status=JobStatus.FAILED,
                error=str(e),
                message="íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨"
            ))
            
            # ì‹¤íŒ¨ ì‹œ íŒŒì¼ ì‚­ì œ
            if 'file_path' in locals() and os.path.exists(file_path):
                os.remove(file_path)
            
            raise
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )


async def extract_file_content(file_path: str, file_ext: str) -> str:
    """íŒŒì¼ ë‚´ìš© ì¶”ì¶œ"""
    try:
        if file_ext in ['.txt', '.md', '.html', '.htm']:
            # í…ìŠ¤íŠ¸ íŒŒì¼
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                return await f.read()
        elif file_ext == '.pdf':
            # PDF íŒŒì¼ ì²˜ë¦¬
            try:
                import PyPDF2
                content = ""
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page in pdf_reader.pages:
                        content += page.extract_text() + "\n"
                return content.strip() if content.strip() else f"PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {os.path.basename(file_path)}"
            except ImportError:
                logger.error("PyPDF2 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return f"PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {os.path.basename(file_path)}"
            except Exception as e:
                logger.error(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                return f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        elif file_ext in ['.doc', '.docx']:
            # Word íŒŒì¼ ì²˜ë¦¬
            try:
                from docx import Document
                doc = Document(file_path)
                content = ""
                for paragraph in doc.paragraphs:
                    content += paragraph.text + "\n"
                
                # í‘œ ë‚´ìš©ë„ ì¶”ì¶œ
                for table in doc.tables:
                    for row in table.rows:
                        for cell in row.cells:
                            content += cell.text + " "
                        content += "\n"
                
                return content.strip() if content.strip() else f"Word íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {os.path.basename(file_path)}"
            except ImportError:
                logger.error("python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return f"Word ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {os.path.basename(file_path)}"
            except Exception as e:
                logger.error(f"Word íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                return f"Word íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}"
    except Exception as e:
        logger.error(f"íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return f"íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"


@router.get(
    "/download/{document_id}",
    summary="íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
    description="ì—…ë¡œë“œëœ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
)
async def download_file(
    document_id: str,
    service: object = Depends(get_vector_store_service)
):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        # ë¬¸ì„œ ì •ë³´ ì¡°íšŒ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ)
        file_path = os.path.join(UPLOAD_DIRECTORY, f"{document_id}.txt")  # ì„ì‹œë¡œ txt í™•ì¥ì
        
        if not os.path.exists(file_path):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        return FileResponse(
            file_path,
            media_type='application/octet-stream',
            filename=f"{document_id}.txt"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )


@router.post(
    "/upload",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ì—…ë¡œë“œ",
    description="ìƒˆ ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."
)
async def upload_document(
    request: UploadDocumentRequest,
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """
    ë¬¸ì„œ ì—…ë¡œë“œ
    
    - **title**: ë¬¸ì„œ ì œëª©
    - **content**: ë¬¸ì„œ ë‚´ìš© (ìµœì†Œ 10ì)
    - **doc_type**: ë¬¸ì„œ íƒ€ì… (website, pdf, text, confluence, jira)
    - **source_url**: ì†ŒìŠ¤ URL (ì„ íƒì‚¬í•­)
    - **site_id**: ê´€ë ¨ ì‚¬ì´íŠ¸ ID (ì„ íƒì‚¬í•­)
    - **metadata**: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)
    """
    try:
        # ì‘ì—… ì¶”ì  ì‹œì‘
        job = job_service.create_job(JobCreate(
            type=JobType.DOCUMENT_UPLOAD,
            site_id=request.site_id,
            metadata={
                "title": request.title,
                "doc_type": request.doc_type.value,
                "content_length": len(request.content)
            }
        ))
        
        try:
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹œì‘
            job_service.update_job(job.id, JobUpdate(
                status=JobStatus.PROCESSING,
                progress=10.0,
                message="ë¬¸ì„œ ê²€ì¦ ì¤‘..."
            ))
            
            # Document ê°ì²´ ìƒì„±
            document = Document(
                id=str(uuid.uuid4()),
                title=request.title,
                content=request.content,
                doc_type=request.doc_type,
                source_url=request.source_url,
                site_id=request.site_id,
                metadata=request.metadata,
                created_at=datetime.now()
            )
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ë²¡í„°í™” ì¤‘
            job_service.update_job(job.id, JobUpdate(
                progress=50.0,
                message="ë²¡í„°í™” ì²˜ë¦¬ ì¤‘..."
            ))
            
            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            doc_id = await service.add_document(document)
            
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ
            job_service.update_job(job.id, JobUpdate(
                status=JobStatus.COMPLETED,
                progress=100.0,
                message="ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ",
                metadata={"document_id": doc_id}
            ))
            
            logger.info(f"ë¬¸ì„œ ì—…ë¡œë“œ ì„±ê³µ: {doc_id}")
            
            return APIResponse(
                success=True,
                message="ë¬¸ì„œ ì—…ë¡œë“œ ì„±ê³µ",
                data={
                    "document_id": doc_id,
                    "job_id": job.id,
                    "title": document.title,
                    "doc_type": document.doc_type.value,
                    "created_at": document.created_at.isoformat()
                }
            )
            
        except Exception as e:
            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸: ì‹¤íŒ¨
            job_service.update_job(job.id, JobUpdate(
                status=JobStatus.FAILED,
                error=str(e),
                message="ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨"
            ))
            raise
        
    except ValueError as e:
        logger.error(f"ë¬¸ì„œ ì—…ë¡œë“œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        )


@router.delete(
    "/{document_id}",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ì‚­ì œ",
    description="ì§€ì •ëœ ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤."
)
async def delete_document(
    document_id: str,
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        success = await service.delete_document(document_id)
        
        if success:
            logger.info(f"ë¬¸ì„œ ì‚­ì œ ì„±ê³µ: {document_id}")
            return APIResponse(
                success=True,
                message="ë¬¸ì„œ ì‚­ì œ ì„±ê³µ",
                data={"document_id": document_id}
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
        )


@router.get(
    "",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ",
    description="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."
)
async def get_documents(
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        # ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        collection_info = await service.get_collection_info()
        
        # ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆë‹¤ë©´ ë¹ˆ ëª©ë¡ ë°˜í™˜
        if collection_info.get("total_chunks", 0) == 0:
            return APIResponse(
                success=True,
                message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
                data={"documents": [], "total": 0}
            )
        
        # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ (ChromaDBì˜ get() ë©”ì„œë“œ ì‚¬ìš©)
        collection = service._collection
        if collection is None:
            return APIResponse(
                success=True,
                message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
                data={"documents": [], "total": 0}
            )
        
        # ëª¨ë“  ë¬¸ì„œ IDì™€ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        results = collection.get()
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        logger.info(f"ChromaDB ì¡°íšŒ ê²°ê³¼: {len(results.get('ids', []))}ê°œì˜ ID")
        logger.info(f"IDs ìƒ˜í”Œ: {results.get('ids', [])[:3]}")
        
        # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
        documents = []
        processed_doc_ids = set()
        
        for i, doc_id in enumerate(results.get("ids", [])):
            metadata = results.get("metadatas", [{}])[i] if i < len(results.get("metadatas", [])) else {}
            
            # ì›ë³¸ ë¬¸ì„œ ID ì¶”ì¶œ (chunk IDì—ì„œ _chunk_X ë¶€ë¶„ ì œê±°)
            original_doc_id = doc_id.split("_chunk_")[0] if "_chunk_" in doc_id else doc_id
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ì œê±°)
            if original_doc_id in processed_doc_ids:
                continue
                
            processed_doc_ids.add(original_doc_id)
            
            document_info = {
                "id": original_doc_id,
                "filename": metadata.get("title", "ì œëª© ì—†ìŒ"),  # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                "type": metadata.get("doc_type", "unknown"),     # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                "created_at": metadata.get("created_at"),
                "updated_at": metadata.get("created_at"),        # í˜„ì¬ëŠ” ìƒì„±ì¼ê³¼ ë™ì¼
                "site_id": metadata.get("site_id"),
                "source_url": metadata.get("source_url"),
                "vector_count": metadata.get("total_chunks", 1), # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                "size": metadata.get("file_size", 0),           # íŒŒì¼ í¬ê¸° (ì—†ìœ¼ë©´ 0)
                "status": "processed",                           # ì¼ë‹¨ ëª¨ë“  ë¬¸ì„œëŠ” processed ìƒíƒœë¡œ
                # ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ í•„ë“œë„ ìœ ì§€
                "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                "doc_type": metadata.get("doc_type", "unknown"),
                "total_chunks": metadata.get("total_chunks", 1)
            }
            
            documents.append(document_info)
        
        # ìƒì„±ì¼ì‹œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        documents.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        
        return APIResponse(
            success=True,
            message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
            data={
                "documents": documents,
                "total": len(documents)
            }
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get(
    "/list",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ",
    description="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."
)
async def list_documents(
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        # ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        collection_info = await service.get_collection_info()
        
        # ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆë‹¤ë©´ ë¹ˆ ëª©ë¡ ë°˜í™˜
        if collection_info.get("total_documents", 0) == 0:
            return APIResponse(
                success=True,
                message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
                data={"documents": [], "total": 0}
            )
        
        # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ (ChromaDBì˜ get() ë©”ì„œë“œ ì‚¬ìš©)
        collection = service._collection
        if collection is None:
            return APIResponse(
                success=True,
                message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
                data={"documents": [], "total": 0}
            )
        
        # ëª¨ë“  ë¬¸ì„œ IDì™€ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        results = collection.get()

                # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        logger.info(f"ChromaDB ì¡°íšŒ ê²°ê³¼: {len(results.get('ids', []))}ê°œì˜ ID")
        logger.info(f"IDs ìƒ˜í”Œ: {results.get('ids', [])[:3]}")
        
        # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
        documents = []
        processed_doc_ids = set()
        
        for i, doc_id in enumerate(results.get("ids", [])):
            metadata = results.get("metadatas", [{}])[i] if i < len(results.get("metadatas", [])) else {}
            
            # ì›ë³¸ ë¬¸ì„œ ID ì¶”ì¶œ (chunk IDì—ì„œ _chunk_X ë¶€ë¶„ ì œê±°)
            original_doc_id = doc_id.split("_chunk_")[0] if "_chunk_" in doc_id else doc_id
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ì œê±°)
            if original_doc_id in processed_doc_ids:
                continue
                
            processed_doc_ids.add(original_doc_id)
            
            document_info = {
                "id": original_doc_id,
                "filename": metadata.get("title", "ì œëª© ì—†ìŒ"),  # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                "type": metadata.get("doc_type", "unknown"),     # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                "created_at": metadata.get("created_at"),
                "updated_at": metadata.get("created_at"),        # í˜„ì¬ëŠ” ìƒì„±ì¼ê³¼ ë™ì¼
                "site_id": metadata.get("site_id"),
                "source_url": metadata.get("source_url"),
                "vector_count": metadata.get("total_chunks", 1), # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í•„ë“œëª…
                "size": metadata.get("file_size", 0),           # íŒŒì¼ í¬ê¸° (ì—†ìœ¼ë©´ 0)
                "status": "processed",                           # ì¼ë‹¨ ëª¨ë“  ë¬¸ì„œëŠ” processed ìƒíƒœë¡œ
                # ë°±ì›Œë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ í•„ë“œë„ ìœ ì§€
                "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                "doc_type": metadata.get("doc_type", "unknown"),
                "total_chunks": metadata.get("total_chunks", 1)
            }
            
            documents.append(document_info)
        
        # ìƒì„±ì¼ì‹œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        documents.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        
        return APIResponse(
            success=True,
            message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
            data={
                "documents": documents,
                "total": len(documents)
            }
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get(
    "/search",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ê²€ìƒ‰",
    description="ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
)
async def search_documents(
    query: str,
    max_results: int = 5,
    similarity_threshold: float = 0.7,
    site_ids: str = None,  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """
    ë¬¸ì„œ ê²€ìƒ‰
    
    - **query**: ê²€ìƒ‰ì–´
    - **max_results**: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)
    - **similarity_threshold**: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.7)
    - **site_ids**: ì‚¬ì´íŠ¸ ID ëª©ë¡ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì„ íƒì‚¬í•­)
    """
    try:
        # site_ids íŒŒì‹±
        site_id_list = None
        if site_ids:
            site_id_list = [s.strip() for s in site_ids.split(",") if s.strip()]
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = await service.search_similar(
            query=query,
            max_results=max_results,
            site_ids=site_id_list,
            similarity_threshold=similarity_threshold
        )
        
        return APIResponse(
            success=True,
            message="ë¬¸ì„œ ê²€ìƒ‰ ì„±ê³µ",
            data={
                "query": query,
                "results": results,
                "total_found": len(results)
            }
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
        )


@router.get(
    "/",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ",
    description="í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì‹œìŠ¤í…œì˜ ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. RDBì™€ Vector DB ì €ì¥ ìƒíƒœë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
)
async def get_documents(
    limit: int = 50,
    offset: int = 0,
    site_id: str = None,
    template_type: str = None,
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """
    ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ìƒíƒœ í¬í•¨)
    
    - **limit**: ì¡°íšŒí•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 50)
    - **offset**: ì‹œì‘ ìœ„ì¹˜ (ê¸°ë³¸ê°’: 0) 
    - **site_id**: ì‚¬ì´íŠ¸ IDë¡œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
    - **template_type**: í…œí”Œë¦¿ ìœ í˜•ìœ¼ë¡œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
    """
    try:
        # Vector DBì—ì„œ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
        collection_info = await service.get_collection_info()
        
        # í•„í„° ì¡°ê±´ êµ¬ì„±
        where_conditions = {}
        if site_id:
            where_conditions["site_id"] = site_id
        if template_type:
            where_conditions["template_type"] = template_type
        
        # Vector DBì˜ ëª¨ë“  ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        all_results = service._collection.get(
            limit=limit + offset,
            include=["metadatas", "documents"],
            where=where_conditions if where_conditions else None
        )
        
        # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™” (chunkë“¤ì„ ë¬¸ì„œ ë‹¨ìœ„ë¡œ ì§‘ê³„)
        documents_map = {}
        
        if all_results["metadatas"]:
            for i, metadata in enumerate(all_results["metadatas"]):
                doc_id = metadata.get("document_id")
                if not doc_id:
                    continue
                

                
                if doc_id not in documents_map:
                    # í…œí”Œë¦¿ ì •ë³´ ë””ë²„ê¹…
                    template_type = metadata.get("template_type")
                    template_version = metadata.get("template_version")

                    
                    documents_map[doc_id] = {
                        "id": doc_id,
                        "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                        "filename": metadata.get("original_filename", metadata.get("title", "íŒŒì¼ëª… ì—†ìŒ")),
                        "type": metadata.get("doc_type", "unknown"),
                        "site_id": metadata.get("site_id"),
                        "created_at": metadata.get("created_at"),
                        "file_size": metadata.get("file_size", 0),
                        "chunk_count": 0,
                        "total_chunks": metadata.get("total_chunks", 0),
                        # í…œí”Œë¦¿ ê´€ë ¨ ì •ë³´
                        "template_type": template_type,
                        "template_version": template_version,
                        "description": metadata.get("description"),
                        "tags": metadata.get("tags"),
                        # í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ìƒíƒœ
                        "storage_status": {
                            "vector_db": True,  # Vector DBì— ìˆìœ¼ë¯€ë¡œ True
                            "rdb": False,  # ê¸°ë³¸ê°’, ì•„ë˜ì—ì„œ í™•ì¸
                            "file_storage": False  # ê¸°ë³¸ê°’, ì•„ë˜ì—ì„œ í™•ì¸
                        }
                    }
                else:
                    # ê¸°ì¡´ ë¬¸ì„œì— í…œí”Œë¦¿ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì—…ë°ì´íŠ¸
                    existing_doc = documents_map[doc_id]
                    current_template_type = existing_doc.get("template_type")
                    new_template_type = metadata.get("template_type")
                    
                    # í˜„ì¬ í…œí”Œë¦¿ íƒ€ì…ì´ ì—†ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ê³ , ìƒˆë¡œìš´ í…œí”Œë¦¿ íƒ€ì…ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                    if (not current_template_type or current_template_type == "") and new_template_type:

                        existing_doc["template_type"] = new_template_type
                        existing_doc["template_version"] = metadata.get("template_version")
                        existing_doc["description"] = metadata.get("description") or existing_doc.get("description")
                        existing_doc["tags"] = metadata.get("tags") or existing_doc.get("tags")
                
                documents_map[doc_id]["chunk_count"] += 1
        
        # ë¬¸ì„œ ëª©ë¡ ìƒì„±
        documents_list = list(documents_map.values())[offset:offset+limit]
        
        # ëª¨ë“  ë¬¸ì„œì— í…œí”Œë¦¿ ê´€ë ¨ í•„ë“œê°€ í¬í•¨ë˜ë„ë¡ ë³´ì¥
        for doc in documents_list:
            # í…œí”Œë¦¿ ê´€ë ¨ í•„ë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
            if 'template_type' not in doc or doc.get('template_type') is None:
                doc['template_type'] = ""
            if 'template_version' not in doc or doc.get('template_version') is None:
                doc['template_version'] = ""
            if 'description' not in doc or doc.get('description') is None:
                doc['description'] = ""
            if 'tags' not in doc or doc.get('tags') is None:
                doc['tags'] = ""
                

        
        # ê° ë¬¸ì„œì˜ RDB ë° íŒŒì¼ ì €ì¥ ìƒíƒœ í™•ì¸
        template_service_instance = get_template_service()
        
        for doc in documents_list:
            # íŒŒì¼ ì €ì¥ ìƒíƒœ í™•ì¸ (íŒŒì¼ ì‹œìŠ¤í…œ)
            file_path = None
            file_exists = False
            
            # ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ íŒŒì¼ ê²½ë¡œ í™•ì¸
            possible_paths = [
                doc.get("file_path"),
                doc.get("source_url") if doc.get("source_url", "").startswith("/") else None,
                # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì„œ IDë¡œ íŒŒì¼ ì°¾ê¸°
                f"{UPLOAD_DIRECTORY}/{doc.get('id')}.md",
                f"{UPLOAD_DIRECTORY}/{doc.get('id')}.txt",
                f"{UPLOAD_DIRECTORY}/{doc.get('id')}.pdf",
            ]
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œë„ íŒŒì¼ ê²½ë¡œ í™•ì¸
            if hasattr(doc, '__iter__') and 'metadata' in doc:
                metadata = doc.get("metadata", {})
                if isinstance(metadata, dict):
                    possible_paths.extend([
                        metadata.get("file_path"),
                        metadata.get("original_file_path"),
                        metadata.get("source_url") if metadata.get("source_url", "").startswith("/") else None
                    ])
            
            # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸
            for path in possible_paths:
                if path and isinstance(path, str):
                    try:
                        if os.path.exists(path):
                            file_path = path
                            file_exists = True
                            break
                    except (OSError, TypeError):
                        continue
            
            # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸° (ë§ˆì§€ë§‰ ì‹œë„)
            if not file_exists and doc.get('id'):
                try:
                    doc_id = doc.get('id')
                    for ext in ['.md', '.txt', '.pdf', '.doc', '.docx', '.html']:
                        pattern_path = f"{UPLOAD_DIRECTORY}/{doc_id}{ext}"
                        if os.path.exists(pattern_path):
                            file_path = pattern_path
                            file_exists = True
                            break
                except (OSError, TypeError):
                    pass
            
            doc["storage_status"]["file_storage"] = file_exists
            doc["storage_status"]["file_path"] = file_path if file_exists else None
            
            # RDB ì €ì¥ ìƒíƒœ í™•ì¸ (ì‹¤ì œ SQLite í…œí”Œë¦¿ í…Œì´ë¸”ì—ì„œ í™•ì¸)
            template_id = doc.get("template_id")
            is_template = doc.get("is_template", False) or doc.get("template_type", "") != ""
            
            if is_template and template_service_instance:
                try:
                    # ë¬¸ì„œ IDë‚˜ í…œí”Œë¦¿ ì´ë¦„ìœ¼ë¡œ SQLiteì—ì„œ í…œí”Œë¦¿ ê²€ìƒ‰
                    doc_id = doc.get("id")
                    template_name = doc.get("template_name") or doc.get("filename", "").rsplit('.', 1)[0]
                    template_type_str = doc.get("template_type", "")
                    
                    # SQLiteì—ì„œ í…œí”Œë¦¿ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    template_exists = False
                    
                    if template_id:
                        # í…œí”Œë¦¿ IDë¡œ ì§ì ‘ ì¡°íšŒ
                        template = await template_service_instance.get_template(template_id)
                        template_exists = template is not None
                    elif template_type_str and template_name:
                        # í…œí”Œë¦¿ íƒ€ì…ê³¼ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
                        from app.domain.entities.template_entities import TemplateType
                        try:
                            template_type_enum = TemplateType(template_type_str)
                            templates = await template_service_instance.get_templates_by_filters(
                                template_type=template_type_enum,
                                site_id=doc.get("site_id"),
                                limit=10
                            )
                            # ì´ë¦„ì´ ìœ ì‚¬í•œ í…œí”Œë¦¿ ì°¾ê¸°
                            for template in templates:
                                if template_name in template.name or template.name in template_name:
                                    template_exists = True
                                    doc["template_id"] = template.id  # í…œí”Œë¦¿ ID ì—…ë°ì´íŠ¸
                                    break
                        except ValueError:
                            # ì˜ëª»ëœ í…œí”Œë¦¿ íƒ€ì…ì¸ ê²½ìš°
                            pass
                    
                    doc["storage_status"]["rdb"] = template_exists
                    
                except Exception as e:
                    logger.warning(f"í…œí”Œë¦¿ ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨ (ë¬¸ì„œ ID: {doc.get('id')}): {e}")
                    doc["storage_status"]["rdb"] = False
            else:
                doc["storage_status"]["rdb"] = False
            
            # ì €ì¥ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€ (ì•„ì´ì½˜ê³¼ í•¨ê»˜)
            storage_locations = []
            storage_locations_with_icons = []
            
            if doc["storage_status"]["vector_db"]:
                storage_locations.append("Vector DB (ChromaDB)")
                storage_locations_with_icons.append("ğŸ” Vector DB")
            if doc["storage_status"]["rdb"]:
                storage_locations.append("SQLite (í…œí”Œë¦¿)")
                storage_locations_with_icons.append("ğŸ—ƒï¸ SQLite")
            if doc["storage_status"]["file_storage"]:
                storage_locations.append("File System")
                storage_locations_with_icons.append("ğŸ“ File System")
            
            doc["storage_status"]["locations"] = storage_locations
            doc["storage_status"]["locations_with_icons"] = storage_locations_with_icons
            doc["storage_status"]["locations_text"] = " + ".join(storage_locations) if storage_locations else "ì—†ìŒ"
            doc["storage_status"]["locations_text_with_icons"] = " + ".join(storage_locations_with_icons) if storage_locations_with_icons else "âŒ ì—†ìŒ"
            
            # ì €ì¥ ìƒíƒœ ìš”ì•½
            storage_count = sum([
                doc["storage_status"]["vector_db"],
                doc["storage_status"]["rdb"], 
                doc["storage_status"]["file_storage"]
            ])
            doc["storage_status"]["total_storages"] = storage_count
            doc["storage_status"]["is_complete"] = storage_count >= 2  # 2ê³³ ì´ìƒ ì €ì¥ë˜ë©´ ì™„ì „í•œ ìƒíƒœ
            doc["storage_status"]["is_hybrid"] = storage_count > 1  # í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ì—¬ë¶€
            
            # ì €ì¥ ìƒíƒœ ë ˆë²¨ (UIì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
            if storage_count == 0:
                doc["storage_status"]["level"] = "none"
                doc["storage_status"]["level_text"] = "ì €ì¥ë˜ì§€ ì•ŠìŒ"
                doc["storage_status"]["level_color"] = "red"
                doc["storage_status"]["level_icon"] = "âŒ"
                doc["storage_status"]["level_description"] = "ë¬¸ì„œê°€ ì–´ë–¤ ì €ì¥ì†Œì—ë„ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            elif storage_count == 1:
                doc["storage_status"]["level"] = "single"
                doc["storage_status"]["level_text"] = "ë‹¨ì¼ ì €ì¥ì†Œ"
                doc["storage_status"]["level_color"] = "yellow"
                doc["storage_status"]["level_icon"] = "âš ï¸"
                doc["storage_status"]["level_description"] = "ë¬¸ì„œê°€ í•˜ë‚˜ì˜ ì €ì¥ì†Œì—ë§Œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
            elif storage_count == 2:
                doc["storage_status"]["level"] = "hybrid"
                doc["storage_status"]["level_text"] = "í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥"
                doc["storage_status"]["level_color"] = "green"
                doc["storage_status"]["level_icon"] = "âœ…"
                doc["storage_status"]["level_description"] = "ë¬¸ì„œê°€ ë‘ ê°œì˜ ì €ì¥ì†Œì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ê¶Œì¥)"
            else:
                doc["storage_status"]["level"] = "full"
                doc["storage_status"]["level_text"] = "ì „ì²´ ì €ì¥"
                doc["storage_status"]["level_color"] = "blue"
                doc["storage_status"]["level_icon"] = "ğŸ¯"
                doc["storage_status"]["level_description"] = "ë¬¸ì„œê°€ ëª¨ë“  ì €ì¥ì†Œì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (ìµœê³ )"
            
            # ì €ì¥ ìƒíƒœë³„ ìƒì„¸ ì •ë³´
            storage_details = {
                "vector_db": {
                    "name": "Vector Database",
                    "description": "ê²€ìƒ‰ ë° ìœ ì‚¬ë„ ë¶„ì„ìš©",
                    "icon": "ğŸ”",
                    "status": "âœ…" if doc["storage_status"]["vector_db"] else "âŒ",
                    "enabled": doc["storage_status"]["vector_db"]
                },
                "rdb": {
                    "name": "SQLite Database", 
                    "description": "í…œí”Œë¦¿ ë©”íƒ€ë°ì´í„° ì €ì¥ìš©",
                    "icon": "ğŸ—ƒï¸",
                    "status": "âœ…" if doc["storage_status"]["rdb"] else "âŒ",
                    "enabled": doc["storage_status"]["rdb"]
                },
                "file_storage": {
                    "name": "File System",
                    "description": "ì›ë³¸ íŒŒì¼ ë³´ê´€ìš©",
                    "icon": "ğŸ“", 
                    "status": "âœ…" if doc["storage_status"]["file_storage"] else "âŒ",
                    "enabled": doc["storage_status"]["file_storage"],
                    "path": doc["storage_status"]["file_path"]
                }
            }
            doc["storage_status"]["details"] = storage_details
            
            # ê¶Œì¥ì‚¬í•­ ì œì‹œ
            recommendations = []
            if not doc["storage_status"]["vector_db"]:
                recommendations.append("Vector DBì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ì„¸ìš”")
            if is_template and not doc["storage_status"]["rdb"]:
                recommendations.append("í…œí”Œë¦¿ì„ SQLiteì— ì €ì¥í•˜ì—¬ í…œí”Œë¦¿ ê´€ë¦¬ ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ì„¸ìš”")
            if not doc["storage_status"]["file_storage"]:
                recommendations.append("ì›ë³¸ íŒŒì¼ì„ íŒŒì¼ ì‹œìŠ¤í…œì— ë°±ì—…í•˜ì„¸ìš”")
            
            doc["storage_status"]["recommendations"] = recommendations
        
        return APIResponse(
            success=True,
            message="ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
            data={
                "documents": documents_list,
                "total_count": len(documents_map),
                "returned_count": len(documents_list),
                "offset": offset,
                "limit": limit,
                "collection_info": {
                    "total_chunks": collection_info.get("total_chunks", 0),
                    "total_sites": collection_info.get("total_sites", 0),
                    "document_types": collection_info.get("document_types", {})
                }
            }
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.get(
    "/stats",
    response_model=APIResponse,
    summary="ë¬¸ì„œ í†µê³„",
    description="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ë¬¸ì„œ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
)
async def get_document_stats(
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """ë¬¸ì„œ í†µê³„ ì¡°íšŒ"""
    try:
        stats = await service.get_collection_info()
        
        return APIResponse(
            success=True,
            message="ë¬¸ì„œ í†µê³„ ì¡°íšŒ ì„±ê³µ",
            data=stats
        )
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


@router.patch(
    "/{document_id}/template-type",
    response_model=APIResponse,
    summary="ë¬¸ì„œ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸",
    description="ê¸°ì¡´ ë¬¸ì„œì˜ í…œí”Œë¦¿ íƒ€ì…ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
)
async def update_document_template_type(
    document_id: str,
    template_type: str = Form(...),
    template_name: str = Form(None),
    template_description: str = Form(None),
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """ê¸°ì¡´ ë¬¸ì„œì˜ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸"""
    try:
        if not service._collection:
            await service.initialize()
        
        # ë¬¸ì„œ ì¡´ì¬ í™•ì¸ ë° í˜„ì¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        existing_results = service._collection.get(
            where={"document_id": document_id},
            include=["metadatas", "documents"]
        )
        
        if not existing_results["ids"]:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}"
            )
        
        # ëª¨ë“  ì²­í¬ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        updated_count = 0
        for i, chunk_id in enumerate(existing_results["ids"]):
            current_metadata = existing_results["metadatas"][i]
            
            # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            updated_metadata = {
                **current_metadata,
                "template_type": template_type,
                "template_name": template_name if template_name else current_metadata.get("template_name", ""),
                "template_description": template_description if template_description else current_metadata.get("template_description", ""),
                "is_template": True,
                "updated_at": datetime.now().isoformat()
            }
            
            # ChromaDBì—ì„œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            service._collection.update(
                ids=[chunk_id],
                metadatas=[updated_metadata]
            )
            updated_count += 1
        
        logger.info(f"ë¬¸ì„œ {document_id}ì˜ í…œí”Œë¦¿ íƒ€ì…ì„ {template_type}ë¡œ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({updated_count}ê°œ ì²­í¬)")
        
        return APIResponse(
            success=True,
            message=f"ë¬¸ì„œ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸ ì™„ë£Œ",
            data={
                "document_id": document_id,
                "template_type": template_type,
                "template_name": template_name,
                "template_description": template_description,
                "updated_chunks": updated_count
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )


@router.post(
    "/batch-update-template-types",
    response_model=APIResponse,
    summary="ì¼ê´„ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸",
    description="ì—¬ëŸ¬ ë¬¸ì„œì˜ í…œí”Œë¦¿ íƒ€ì…ì„ ì¼ê´„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
)
async def batch_update_template_types(
    updates: List[Dict[str, str]],  # [{"document_id": "...", "template_type": "...", "template_name": "...", "template_description": "..."}]
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """ì—¬ëŸ¬ ë¬¸ì„œì˜ í…œí”Œë¦¿ íƒ€ì…ì„ ì¼ê´„ ì—…ë°ì´íŠ¸"""
    try:
        if not service._collection:
            await service.initialize()
        
        results = []
        
        for update in updates:
            document_id = update.get("document_id")
            template_type = update.get("template_type")
            template_name = update.get("template_name", "")
            template_description = update.get("template_description", "")
            
            if not document_id or not template_type:
                results.append({
                    "document_id": document_id,
                    "status": "failed",
                    "error": "document_idì™€ template_typeì€ í•„ìˆ˜ì…ë‹ˆë‹¤"
                })
                continue
            
            try:
                # ë¬¸ì„œ ì¡´ì¬ í™•ì¸ ë° í˜„ì¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
                existing_results = service._collection.get(
                    where={"document_id": document_id},
                    include=["metadatas"]
                )
                
                if not existing_results["ids"]:
                    results.append({
                        "document_id": document_id,
                        "status": "failed",
                        "error": "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    })
                    continue
                
                # ëª¨ë“  ì²­í¬ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                updated_count = 0
                for i, chunk_id in enumerate(existing_results["ids"]):
                    current_metadata = existing_results["metadatas"][i]
                    
                    # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    updated_metadata = {
                        **current_metadata,
                        "template_type": template_type,
                        "template_name": template_name,
                        "template_description": template_description,
                        "is_template": True,
                        "updated_at": datetime.now().isoformat()
                    }
                    
                    # ChromaDBì—ì„œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    service._collection.update(
                        ids=[chunk_id],
                        metadatas=[updated_metadata]
                    )
                    updated_count += 1
                
                results.append({
                    "document_id": document_id,
                    "status": "success",
                    "template_type": template_type,
                    "updated_chunks": updated_count
                })
                
            except Exception as e:
                results.append({
                    "document_id": document_id,
                    "status": "failed",
                    "error": str(e)
                })
        
        success_count = len([r for r in results if r["status"] == "success"])
        failed_count = len([r for r in results if r["status"] == "failed"])
        
        return APIResponse(
            success=True,
            message=f"ì¼ê´„ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failed_count})",
            data={
                "total": len(updates),
                "success_count": success_count,
                "failed_count": failed_count,
                "results": results
            }
        )
        
    except Exception as e:
        logger.error(f"ì¼ê´„ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¼ê´„ í…œí”Œë¦¿ íƒ€ì… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )


@router.get(
    "/{document_id}/storage-status",
    response_model=APIResponse,
    summary="ë¬¸ì„œ ì €ì¥ ìƒíƒœ ìƒì„¸ ì¡°íšŒ",
    description="íŠ¹ì • ë¬¸ì„œì˜ í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥ ìƒíƒœë¥¼ ìƒì„¸í•˜ê²Œ ì¡°íšŒí•©ë‹ˆë‹¤."
)
async def get_document_storage_status(
    document_id: str,
    service: object = Depends(get_vector_store_service)
) -> APIResponse:
    """íŠ¹ì • ë¬¸ì„œì˜ ì €ì¥ ìƒíƒœ ìƒì„¸ ì¡°íšŒ"""
    try:
        if not service._collection:
            await service.initialize()
        
        # Vector DBì—ì„œ ë¬¸ì„œ ì¡°íšŒ
        results = service._collection.get(
            where={"document_id": document_id},
            include=["metadatas", "documents"]
        )
        
        if not results["ids"]:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}"
            )
        
        # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš© (ë¬¸ì„œ ì •ë³´ëŠ” ëª¨ë“  ì²­í¬ì— ë™ì¼)
        metadata = results["metadatas"][0]
        
        # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
        doc_info = {
            "id": document_id,
            "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
            "filename": metadata.get("original_filename", metadata.get("title", "íŒŒì¼ëª… ì—†ìŒ")),
            "type": metadata.get("doc_type", "unknown"),
            "site_id": metadata.get("site_id"),
            "created_at": metadata.get("created_at"),
            "template_type": metadata.get("template_type", ""),
            "template_name": metadata.get("template_name", ""),
            "is_template": metadata.get("is_template", False),
            "total_chunks": len(results["ids"])
        }
        
        # ì €ì¥ ìƒíƒœ í™•ì¸ (ìœ„ì—ì„œ êµ¬í˜„í•œ ë¡œì§ê³¼ ë™ì¼)
        template_service_instance = get_template_service()
        
        # Vector DB ìƒíƒœ (ì´ë¯¸ ì¡°íšŒí–ˆìœ¼ë¯€ë¡œ True)
        storage_status = {"vector_db": True}
        
        # íŒŒì¼ ì €ì¥ ìƒíƒœ í™•ì¸
        file_path = metadata.get("file_path")
        file_exists = False
        
        if file_path and os.path.exists(file_path):
            file_exists = True
        else:
            # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸°
            for ext in ['.md', '.txt', '.pdf', '.doc', '.docx', '.html']:
                pattern_path = f"{UPLOAD_DIRECTORY}/{document_id}{ext}"
                if os.path.exists(pattern_path):
                    file_path = pattern_path
                    file_exists = True
                    break
        
        storage_status["file_storage"] = file_exists
        storage_status["file_path"] = file_path if file_exists else None
        
        # RDB ì €ì¥ ìƒíƒœ í™•ì¸
        is_template = doc_info.get("is_template", False) or doc_info.get("template_type", "") != ""
        template_exists = False
        
        if is_template and template_service_instance:
            try:
                template_type_str = doc_info.get("template_type", "")
                template_name = doc_info.get("template_name") or doc_info.get("filename", "").rsplit('.', 1)[0]
                
                if template_type_str and template_name:
                    from app.domain.entities.template_entities import TemplateType
                    try:
                        template_type_enum = TemplateType(template_type_str)
                        templates = await template_service_instance.get_templates_by_filters(
                            template_type=template_type_enum,
                            site_id=doc_info.get("site_id"),
                            limit=10
                        )
                        for template in templates:
                            if template_name in template.name or template.name in template_name:
                                template_exists = True
                                break
                    except ValueError:
                        pass
            except Exception as e:
                logger.warning(f"í…œí”Œë¦¿ ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        storage_status["rdb"] = template_exists
        
        # ì €ì¥ ìƒíƒœ ìš”ì•½ ì •ë³´ ìƒì„± (ìœ„ì—ì„œ êµ¬í˜„í•œ ë¡œì§ ì¬ì‚¬ìš©)
        storage_count = sum([
            storage_status["vector_db"],
            storage_status["rdb"],
            storage_status["file_storage"]
        ])
        
        # ìƒíƒœ ë ˆë²¨ ê²°ì •
        if storage_count == 0:
            level = "none"
            level_text = "ì €ì¥ë˜ì§€ ì•ŠìŒ"
            level_color = "red"
            level_icon = "âŒ"
        elif storage_count == 1:
            level = "single"
            level_text = "ë‹¨ì¼ ì €ì¥ì†Œ"
            level_color = "yellow"
            level_icon = "âš ï¸"
        elif storage_count == 2:
            level = "hybrid"
            level_text = "í•˜ì´ë¸Œë¦¬ë“œ ì €ì¥"
            level_color = "green"
            level_icon = "âœ…"
        else:
            level = "full"
            level_text = "ì „ì²´ ì €ì¥"
            level_color = "blue"
            level_icon = "ğŸ¯"
        
        # ì €ì¥ ìœ„ì¹˜ ì •ë³´
        locations = []
        if storage_status["vector_db"]:
            locations.append("ğŸ” Vector DB")
        if storage_status["rdb"]:
            locations.append("ğŸ—ƒï¸ SQLite")
        if storage_status["file_storage"]:
            locations.append("ğŸ“ File System")
        
        return APIResponse(
            success=True,
            message="ë¬¸ì„œ ì €ì¥ ìƒíƒœ ì¡°íšŒ ì„±ê³µ",
            data={
                "document": doc_info,
                "storage_status": {
                    **storage_status,
                    "total_storages": storage_count,
                    "is_hybrid": storage_count > 1,
                    "is_complete": storage_count >= 2,
                    "level": level,
                    "level_text": level_text,
                    "level_color": level_color,
                    "level_icon": level_icon,
                    "locations": locations,
                    "locations_text": " + ".join(locations) if locations else "âŒ ì—†ìŒ"
                }
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì €ì¥ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ì €ì¥ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        ) 