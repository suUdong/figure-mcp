# LangChain ê¸°ë°˜ figure-backend ì„¤ê³„

## ğŸ¯ LangChain í†µí•© ëª©ì 
- ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ ìœ ì—°í•˜ê²Œ í™œìš©
- RAG íŒŒì´í”„ë¼ì¸ì˜ ì²´ê³„ì  êµ¬ì„±
- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì²´ì¸ìœ¼ë¡œ ê´€ë¦¬
- ë„êµ¬ì™€ ì—ì´ì „íŠ¸ì˜ í†µí•© ê´€ë¦¬

## ğŸ”— ì£¼ìš” LangChain ì»´í¬ë„ŒíŠ¸

### 1. LLM í”„ë¡œë°”ì´ë” ê´€ë¦¬
```python
from langchain.llms import OpenAI, AzureOpenAI
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.llms import Ollama  # ë¡œì»¬ LLM

class LLMManager:
    def __init__(self):
        self.providers = {
            "openai": ChatOpenAI(model="gpt-4"),
            "claude": ChatAnthropic(model="claude-3-sonnet-20240229"),
            "azure": AzureOpenAI(deployment_name="gpt-4"),
            "local": Ollama(model="llama2")  # ë¡œì»¬ LLM
        }
    
    def get_llm(self, provider: str, task_type: str):
        """ì‘ì—… íƒ€ì…ì— ë”°ë¼ ìµœì ì˜ LLM ì„ íƒ"""
        if task_type == "requirements_analysis":
            return self.providers["claude"]  # ê¸´ ë¬¸ì„œ ë¶„ì„ì— ì¢‹ìŒ
        elif task_type == "code_analysis":
            return self.providers["openai"]   # ì½”ë“œ ì´í•´ì— ì¢‹ìŒ
        elif task_type == "summarization":
            return self.providers["local"]    # ë¹„ìš© ì ˆì•½
        else:
            return self.providers["openai"]   # ê¸°ë³¸ê°’
```

### 2. ì»¤ìŠ¤í…€ ë„êµ¬ (Tools) êµ¬í˜„
```python
from langchain.tools import BaseTool
from typing import Optional, Type
from pydantic import BaseModel, Field

class JiraTicketTool(BaseTool):
    name = "jira_ticket_fetcher"
    description = "JIRA í‹°ì¼“ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"
    
    def _run(self, ticket_id: str) -> str:
        """JIRA APIë¥¼ í†µí•´ í‹°ì¼“ ì •ë³´ ìˆ˜ì§‘"""
        ticket_info = fetch_jira_ticket(ticket_id)
        return f"í‹°ì¼“ {ticket_id}: {ticket_info['summary']}\nì„¤ëª…: {ticket_info['description']}"

class VectorSearchTool(BaseTool):
    name = "vector_search"
    description = "ì‚¬ì´íŠ¸ë³„ í‘œì¤€ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬"
    
    def _run(self, query: str, site_id: str, top_k: int = 5) -> str:
        """Vector DBì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        results = vector_search(site_id, query, top_k)
        return format_search_results(results)

class TemplateTool(BaseTool):
    name = "template_fetcher"
    description = "ë¬¸ì„œ íƒ€ì…ë³„ í…œí”Œë¦¿ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬"
    
    def _run(self, site_id: str, doc_type: str) -> str:
        """í…œí”Œë¦¿ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í…œí”Œë¦¿ ì¡°íšŒ"""
        template = get_template(site_id, doc_type)
        return template["content"]
```

### 3. ì²´ì¸ (Chains) êµ¬í˜„

#### ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ ì²´ì¸
```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, AgentType

class RequirementsChain:
    def __init__(self, llm_manager: LLMManager):
        self.llm = llm_manager.get_llm("claude", "requirements_analysis")
        self.tools = [
            JiraTicketTool(),
            VectorSearchTool(),
            TemplateTool()
        ]
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
    
    async def generate_requirements_guide(self, site_id: str, ticket_id: str):
        prompt = f"""
        ì‚¬ì´íŠ¸ {site_id}ì˜ JIRA í‹°ì¼“ {ticket_id}ì— ëŒ€í•œ ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ ì‘ì„± ê°€ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
        
        ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ì§„í–‰í•˜ì„¸ìš”:
        1. JIRA í‹°ì¼“ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”
        2. ê´€ë ¨ëœ ì‚¬ì´íŠ¸ í‘œì¤€ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”
        3. í•´ë‹¹ ë¬¸ì„œ íƒ€ì…ì˜ í…œí”Œë¦¿ì„ ê°€ì ¸ì˜¤ì„¸ìš”
        4. ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ êµ¬ì²´ì ì¸ ì‘ì„± ê°€ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”
        
        ìµœì¢… ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
        - í…œí”Œë¦¿ êµ¬ì¡°
        - ì‘ì„± ê°€ì´ë“œë¼ì¸
        - ì²´í¬ë¦¬ìŠ¤íŠ¸
        - ì£¼ì˜ì‚¬í•­
        """
        
        result = await self.agent.arun(prompt)
        return self.parse_result(result)
```

#### ê°œë°œ ì‚°ì¶œë¬¼ ì²´ì¸
```python
class DeliverableChain:
    def __init__(self, llm_manager: LLMManager):
        self.llm = llm_manager.get_llm("openai", "code_analysis")
        self.tools = [VectorSearchTool(), TemplateTool()]
        
        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.llm,
            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION
        )
    
    async def generate_deliverable_guide(self, site_id: str, deliverable_type: str, source_code: str):
        prompt = f"""
        ì‚¬ì´íŠ¸ {site_id}ì—ì„œ {deliverable_type} íƒ€ì…ì˜ ê°œë°œ ì‚°ì¶œë¬¼ ì‘ì„± ê°€ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
        
        ì œê³µëœ ì†ŒìŠ¤ ì½”ë“œ:
        ```
        {source_code}
        ```
        
        ë‹¨ê³„:
        1. ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” ê¸°ëŠ¥ê³¼ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì„¸ìš”
        2. í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ {deliverable_type} ê´€ë ¨ í‘œì¤€ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”
        3. ì ì ˆí•œ í…œí”Œë¦¿ì„ ê°€ì ¸ì˜¤ì„¸ìš”
        4. ì½”ë“œ ë¶„ì„ ê²°ê³¼ì™€ í‘œì¤€ì„ ê²°í•©í•˜ì—¬ ë§ì¶¤í˜• ê°€ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”
        """
        
        result = await self.agent.arun(prompt)
        return self.parse_result(result)
```

### 4. ë©”ëª¨ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
```python
from langchain.memory import ConversationBufferWindowMemory
from langchain.memory.chat_message_histories import RedisChatMessageHistory

class ContextManager:
    def __init__(self, redis_client):
        self.redis_client = redis_client
    
    def get_memory(self, session_id: str):
        """ì„¸ì…˜ë³„ ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬"""
        message_history = RedisChatMessageHistory(
            session_id=session_id,
            url="redis://localhost:6379"
        )
        
        return ConversationBufferWindowMemory(
            k=10,  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ ìœ ì§€
            chat_memory=message_history,
            return_messages=True
        )
```

### 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬
```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate

class PromptManager:
    def __init__(self):
        self.templates = {
            "requirements_analysis": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                ì‚¬ì´íŠ¸ë³„ ê°œë°œ í‘œì¤€ì— ë§ëŠ” ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ ì‘ì„±ì„ ë„ì™€ì£¼ì„¸ìš”."""),
                ("human", "{input}")
            ]),
            
            "api_documentation": PromptTemplate(
                input_variables=["code", "standards", "template"],
                template="""
                ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ API ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:
                
                ì½”ë“œ:
                {code}
                
                í‘œì¤€ ê°€ì´ë“œë¼ì¸:
                {standards}
                
                í…œí”Œë¦¿:
                {template}
                
                ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•œ API ë¬¸ì„œ ì‘ì„± ê°€ì´ë“œë¥¼ ì œê³µí•˜ì„¸ìš”.
                """
            )
        }
    
    def get_template(self, template_name: str):
        return self.templates.get(template_name)
```

## ğŸ”§ FastAPIì™€ LangChain í†µí•©

### API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

# LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
llm_manager = LLMManager()
requirements_chain = RequirementsChain(llm_manager)
deliverable_chain = DeliverableChain(llm_manager)
context_manager = ContextManager(redis_client)

class RequirementsRequest(BaseModel):
    site_id: str
    ticket_id: str
    session_id: Optional[str] = None

@app.post("/api/rag/requirements")
async def generate_requirements_guide(request: RequirementsRequest):
    try:
        # ì„¸ì…˜ ë©”ëª¨ë¦¬ ì„¤ì •
        if request.session_id:
            memory = context_manager.get_memory(request.session_id)
            requirements_chain.agent.memory = memory
        
        # ìš”êµ¬ì‚¬í•­ ê°€ì´ë“œ ìƒì„±
        result = await requirements_chain.generate_requirements_guide(
            request.site_id, 
            request.ticket_id
        )
        
        return {"success": True, "data": result}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class DeliverableRequest(BaseModel):
    site_id: str
    deliverable_type: str
    source_code: str
    session_id: Optional[str] = None

@app.post("/api/rag/deliverable")
async def generate_deliverable_guide(request: DeliverableRequest):
    try:
        result = await deliverable_chain.generate_deliverable_guide(
            request.site_id,
            request.deliverable_type,
            request.source_code
        )
        
        return {"success": True, "data": result}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## ğŸ“Š ì„¤ì • ë° í™˜ê²½ ê´€ë¦¬
```python
# config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # LLM API í‚¤
    openai_api_key: str
    anthropic_api_key: str
    azure_openai_key: str
    
    # ë°ì´í„°ë² ì´ìŠ¤
    database_url: str
    redis_url: str
    chroma_host: str
    
    # LangChain ì„¤ì •
    langchain_tracing: bool = True
    langchain_project: str = "figure-backend"
    
    class Config:
        env_file = ".env"

settings = Settings()
```

## ğŸš€ ë°°í¬ ê³ ë ¤ì‚¬í•­

### Docker ì„¤ì •
```dockerfile
# Dockerfile for figure-backend
FROM python:3.11-slim

WORKDIR /app

# LangChain ë° ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONPATH=/app
ENV LANGCHAIN_TRACING_V2=true

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### requirements.txt
```
fastapi==0.104.1
uvicorn[standard]==0.24.0
langchain==0.1.0
langchain-openai==0.0.2
langchain-anthropic==0.0.1
langchain-community==0.0.10
chromadb==0.4.18
redis==5.0.1
sqlalchemy==2.0.23
pydantic-settings==2.1.0
python-multipart==0.0.6
```

ì´ë ‡ê²Œ LangChainì„ í†µí•©í•˜ë©´ ë” ê°•ë ¥í•˜ê³  ìœ ì—°í•œ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë³´ì‹œë‚˜ìš”? 