#!/usr/bin/env python3
"""
ì‹¤ì œ MD ë¬¸ì„œë¥¼ ì‚¬ìš©í•œ Embedding í…ŒìŠ¤íŠ¸
test-sample í´ë”ì˜ MD íŒŒì¼ë“¤ì„ embeddingí•˜ì—¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import asyncio
import time
import json
from pathlib import Path

# ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/app')

from app.config import Settings
from app.infrastructure.adapters.embeddings.factory import embedding_factory


class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ í´ë˜ìŠ¤"""
    def __init__(self, text: str, source: str, chunk_id: int):
        self.text = text
        self.source = source
        self.chunk_id = chunk_id
        self.embedding = None
        
    def __repr__(self):
        return f"Chunk({self.source}:{self.chunk_id}, {len(self.text)} chars)"


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200):
    """í…ìŠ¤íŠ¸ë¥¼ ê²¹ì¹˜ëŠ” ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    start = 0
    chunk_id = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk_text = text[start:end]
        
        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
        if end < len(text):
            last_period = chunk_text.rfind('.')
            last_newline = chunk_text.rfind('\n')
            boundary = max(last_period, last_newline)
            
            if boundary > start + chunk_size // 2:  # ì²­í¬ì˜ ì ˆë°˜ ì´ìƒì¼ ë•Œë§Œ
                end = start + boundary + 1
                chunk_text = text[start:end]
        
        if chunk_text.strip():
            chunks.append(chunk_text.strip())
            chunk_id += 1
        
        start = end - overlap if end < len(text) else end
    
    return chunks


async def load_and_process_documents():
    """test-sample í´ë”ì˜ MD íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬"""
    print('ğŸ“š ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹ ì²˜ë¦¬...')
    
    # MD íŒŒì¼ ëª©ë¡
    md_files = [
        'technical_documentation.md',
        'troubleshooting_guide.md', 
        'project_management.md',
        'work_instructions.md',
        'impact_analysis_work_instructions.md'
    ]
    
    all_chunks = []
    
    for filename in md_files:
        print(f'   ğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}')
        
        # íŒŒì¼ ë‚´ìš© ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ì½ì–´ì˜¬ í…ìŠ¤íŠ¸)
        if filename == 'technical_documentation.md':
            content = """
# ê¸°ìˆ  ë¬¸ì„œ ë° ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

## ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
ìš°ë¦¬ ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- API Gateway: í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ ë¼ìš°íŒ…í•˜ê³  ì¸ì¦ì„ ì²˜ë¦¬
- User Service: ì‚¬ìš©ì ê´€ë¦¬ ë° ì¸ì¦ ì„œë¹„ìŠ¤
- Payment Service: ê²°ì œ ì²˜ë¦¬ ë° íŠ¸ëœì­ì…˜ ê´€ë¦¬

## API ì„¤ê³„ ì›ì¹™
RESTful API ê°€ì´ë“œë¼ì¸ì„ ë”°ë¥´ë©°, í‘œì¤€í™”ëœ ì‘ë‹µ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

## ë³´ì•ˆ ê°€ì´ë“œë¼ì¸
JWT í† í° ê¸°ë°˜ ì¸ì¦ê³¼ Role-based Access Controlì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
"""
        elif filename == 'troubleshooting_guide.md':
            content = """
# ì‹œìŠ¤í…œ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ì„œë²„ ì—°ê²° ë¬¸ì œ
502 Bad Gateway ì—ëŸ¬ê°€ ë°œìƒí•  ë•ŒëŠ” ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ë¡œê·¸ë¥¼ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.

## ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ì €í•˜
ì¿¼ë¦¬ ì‘ë‹µ ì‹œê°„ì´ ì¦ê°€í•˜ëŠ” ê²½ìš° ì¸ë±ìŠ¤ ìµœì í™”ì™€ ì»¤ë„¥ì…˜ í’€ ì„¤ì •ì„ ì ê²€í•´ì•¼ í•©ë‹ˆë‹¤.

## ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¬¸ì œ
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ê²½ìš° í™ ë¤í”„ë¥¼ ë¶„ì„í•˜ê³  GC ì„¤ì •ì„ ìµœì í™”í•´ì•¼ í•©ë‹ˆë‹¤.
"""
        elif filename == 'project_management.md':
            content = """
# í”„ë¡œì íŠ¸ ê´€ë¦¬ ë° í˜‘ì—… ê°€ì´ë“œ

## ì• ìì¼ ìŠ¤í¬ëŸ¼ ì ìš©
2ì£¼ ë‹¨ìœ„ ìŠ¤í”„ë¦°íŠ¸ë¡œ ì§„í–‰í•˜ë©° ì¼ì¼ ìŠ¤íƒ ë“œì—… ë¯¸íŒ…ì„ í†µí•´ ì§„í–‰ ìƒí™©ì„ ê³µìœ í•©ë‹ˆë‹¤.

## íŒ€ ì—­í•  ë° ì±…ì„
í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ëŠ” ì „ì²´ ì¼ì •ì„ ê´€ë¦¬í•˜ê³ , ê°œë°œíŒ€ ë¦¬ë“œëŠ” ê¸°ìˆ ì  ì˜ì‚¬ê²°ì •ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

## ë¦¬ìŠ¤í¬ ê´€ë¦¬
í•µì‹¬ ê°œë°œì ì´ì§, ìš”êµ¬ì‚¬í•­ ë³€ê²½, ê¸°ìˆ ì  ë‚œì´ë„ ë“±ì˜ ë¦¬ìŠ¤í¬ë¥¼ ì‚¬ì „ì— ì‹ë³„í•˜ê³  ëŒ€ì‘í•©ë‹ˆë‹¤.
"""
        else:
            content = f"""
# {filename.replace('.md', '').replace('_', ' ').title()}

ì´ê²ƒì€ {filename} íŒŒì¼ì˜ ìƒ˜í”Œ ë‚´ìš©ì…ë‹ˆë‹¤.
ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì€ íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì™€ì„œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

## ì£¼ìš” ë‚´ìš©
- ì—…ë¬´ ì§€ì‹œì‚¬í•­
- ì‘ì—… ì ˆì°¨
- í’ˆì§ˆ ê´€ë¦¬
- ë¬¸ì œ í•´ê²° ë°©ë²•
"""
        
        # í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = chunk_text(content)
        
        for i, chunk_text in enumerate(chunks):
            chunk = DocumentChunk(chunk_text, filename, i)
            all_chunks.append(chunk)
        
        print(f'      â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±')
    
    print(f'âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ì¤€ë¹„ ì™„ë£Œ')
    return all_chunks


async def embed_documents(chunks, adapter):
    """ë¬¸ì„œ ì²­í¬ë“¤ì„ embedding"""
    print('\nğŸ”® ë¬¸ì„œ Embedding ìˆ˜í–‰...')
    
    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    texts = [chunk.text for chunk in chunks]
    
    start_time = time.time()
    
    # ë°°ì¹˜ë¡œ embedding ìˆ˜í–‰ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ì€ ë°°ì¹˜ë¡œ)
    batch_size = 5
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        print(f'   ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...')
        
        batch_embeddings = await adapter.aembed_documents(batch_texts)
        all_embeddings.extend(batch_embeddings)
        
        # í”„ë¡œê·¸ë ˆìŠ¤ í‘œì‹œ
        progress = min(i + batch_size, len(texts))
        print(f'      â†’ {progress}/{len(texts)} ì™„ë£Œ')
    
    total_time = time.time() - start_time
    
    # ì²­í¬ì— embedding í• ë‹¹
    for chunk, embedding in zip(chunks, all_embeddings):
        chunk.embedding = embedding
    
    print(f'âœ… Embedding ì™„ë£Œ!')
    print(f'   â±ï¸ ì´ ì‹œê°„: {total_time:.3f}ì´ˆ')
    print(f'   ğŸ“Š í‰ê·  ì‹œê°„: {total_time/len(chunks):.3f}ì´ˆ/ì²­í¬')
    print(f'   ğŸ“ Embedding ì°¨ì›: {len(all_embeddings[0])}')
    
    return chunks


def cosine_similarity(a, b):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    import math
    dot_product = sum(x * y for x, y in zip(a, b))
    magnitude_a = math.sqrt(sum(x * x for x in a))
    magnitude_b = math.sqrt(sum(x * x for x in b))
    return dot_product / (magnitude_a * magnitude_b)


async def semantic_search(query, chunks, adapter, top_k=5):
    """ì˜ë¯¸ì  ê²€ìƒ‰ ìˆ˜í–‰"""
    print(f'\nğŸ” ì˜ë¯¸ì  ê²€ìƒ‰: "{query}"')
    
    # ì¿¼ë¦¬ embedding
    query_embedding = await adapter.aembed_query(query)
    
    # ëª¨ë“  ì²­í¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for chunk in chunks:
        similarity = cosine_similarity(query_embedding, chunk.embedding)
        similarities.append((chunk, similarity))
    
    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
    top_results = similarities[:top_k]
    
    print(f'ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {top_k}ê°œ):')
    for i, (chunk, score) in enumerate(top_results, 1):
        print(f'   {i}. {chunk.source}:{chunk.chunk_id} (ìœ ì‚¬ë„: {score:.4f})')
        print(f'      "{chunk.text[:100]}..."')
        print()
    
    return top_results


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print('ğŸš€ Figure Backend MD ë¬¸ì„œ Embedding í…ŒìŠ¤íŠ¸')
    print('=' * 60)
    
    try:
        # Provider ì„¤ì • (API í‚¤ê°€ ìˆëŠ” ê²ƒ ì‚¬ìš©)
        settings = Settings()
        available_providers = embedding_factory.get_available_providers()
        
        working_provider = None
        for provider in available_providers:
            settings.embedding_provider = provider
            
            # API í‚¤ í™•ì¸
            if provider == 'gemini' and settings.gemini_api_key:
                working_provider = provider
                break
            elif provider == 'openai' and settings.openai_api_key:
                working_provider = provider
                break
            elif provider == 'voyage' and settings.voyage_api_key:
                working_provider = provider
                break
        
        if not working_provider:
            print('âŒ API í‚¤ê°€ ì„¤ì •ëœ providerê°€ ì—†ìŠµë‹ˆë‹¤')
            return False
        
        print(f'ğŸ”§ ì‚¬ìš© ì¤‘ì¸ Provider: {working_provider.upper()}')
        
        # ì–´ëŒ‘í„° ìƒì„±
        adapter = embedding_factory.create_adapter(settings)
        
        # ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬
        chunks = await load_and_process_documents()
        
        # ë¬¸ì„œ embedding
        embedded_chunks = await embed_documents(chunks, adapter)
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        test_queries = [
            "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì„œë²„ ì—°ê²° ì˜¤ë¥˜ë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
            "í”„ë¡œì íŠ¸ ê´€ë¦¬ ë°©ë²•ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¬¸ì œ í•´ê²°",
            "JWT í† í° ì¸ì¦ ë°©ë²•"
        ]
        
        print('\nğŸ¯ ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸')
        print('=' * 40)
        
        for query in test_queries:
            await semantic_search(query, embedded_chunks, adapter, top_k=3)
            print('-' * 40)
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'provider': working_provider,
            'total_chunks': len(embedded_chunks),
            'embedding_dimension': len(embedded_chunks[0].embedding),
            'test_queries': test_queries,
            'documents_processed': list(set(chunk.source for chunk in embedded_chunks))
        }
        
        with open('/app/md_embedding_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print('ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ /app/md_embedding_test_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤')
        print('\nğŸ‰ MD ë¬¸ì„œ Embedding í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
        
        return True
        
    except Exception as e:
        print(f'\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}')
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    print(f'\nğŸ“Š ìµœì¢… ê²°ê³¼: {"SUCCESS" if success else "FAILED"}')
    exit(0 if success else 1) 