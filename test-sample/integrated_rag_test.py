#!/usr/bin/env python3
"""
í†µí•© RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
ì„ë² ë”©(Gemini) + LLM(Claude)ì„ ì‚¬ìš©í•œ ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
"""

import sys
import asyncio
import time
import json
from pathlib import Path

# ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/app')

from app.config import Settings
from app.infrastructure.adapters.embeddings.factory import embedding_factory
from app.infrastructure.adapters.llm.factory import llm_factory
from app.application.services.rag_service import rag_service


async def test_integrated_rag_system():
    """í†µí•© RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print('ğŸš€ Figure Backend í†µí•© RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸')
    print('=' * 70)
    
    try:
        # ì„¤ì • í™•ì¸
        settings = Settings()
        print(f'ğŸ“ LLM Provider: {settings.llm_provider}')
        print(f'ğŸ”® Embedding Provider: {settings.embedding_provider}')
        print(f'ğŸ¤– LLM Model: {getattr(settings, f"{settings.llm_provider}_model", "Unknown")}')
        print(f'ğŸ“Š Embedding Model: {getattr(settings, f"{settings.embedding_provider}_embedding_model", "Unknown")}')
        
        print('\n' + '='*70)
        
        # í…ŒìŠ¤íŠ¸ 1: ê°œë³„ ì»´í¬ë„ŒíŠ¸ í™•ì¸
        print('ğŸ” í…ŒìŠ¤íŠ¸ 1: ê°œë³„ ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸')
        print('-' * 50)
        
        # ì„ë² ë”© ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸
        try:
            embedding_adapter = embedding_factory.create_adapter(settings)
            print(f'âœ… ì„ë² ë”© ì–´ëŒ‘í„°: {embedding_adapter.provider_name} - {embedding_adapter.model_name}')
            
            # ê°„ë‹¨í•œ ì„ë² ë”© í…ŒìŠ¤íŠ¸
            test_text = "ì´ê²ƒì€ ì„ë² ë”© í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            embedding = await embedding_adapter.aembed_query(test_text)
            print(f'   ğŸ“ ì„ë² ë”© ì°¨ì›: {len(embedding)}')
            
            embedding_test_success = True
        except Exception as e:
            print(f'âŒ ì„ë² ë”© ì–´ëŒ‘í„° ì‹¤íŒ¨: {e}')
            embedding_test_success = False
        
        # LLM ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸
        try:
            llm_adapter = llm_factory.create_adapter(settings)
            print(f'âœ… LLM ì–´ëŒ‘í„°: {llm_adapter.provider_name} - {llm_adapter.model_name}')
            
            # ê°„ë‹¨í•œ LLM í…ŒìŠ¤íŠ¸
            response = await llm_adapter.generate_response("ì•ˆë…•í•˜ì„¸ìš”!")
            print(f'   ğŸ“ LLM ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì')
            
            llm_test_success = True
        except Exception as e:
            print(f'âŒ LLM ì–´ëŒ‘í„° ì‹¤íŒ¨: {e}')
            llm_test_success = False
        
        if not (embedding_test_success and llm_test_success):
            print('\nâŒ ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. RAG í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.')
            return False
        
        print('\n' + '='*70)
        
        # í…ŒìŠ¤íŠ¸ 2: RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        print('ğŸ” í…ŒìŠ¤íŠ¸ 2: RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”')
        print('-' * 50)
        
        try:
            await rag_service.initialize()
            status = await rag_service.get_service_status()
            
            print(f'âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: {status["rag_service_initialized"]}')
            print(f'ğŸ”§ LLM Provider: {status.get("llm_provider", "Unknown")}')
            print(f'ğŸ¤– LLM Model: {status.get("llm_model", "Unknown")}')
            print(f'ğŸ“Š ì„ë² ë”© Provider: {status.get("embedding_provider", "Unknown")}')
            print(f'ğŸ—„ï¸  ë²¡í„° ìŠ¤í† ì–´: {status.get("vector_store_initialized", False)}')
            
            rag_init_success = status["rag_service_initialized"]
            
        except Exception as e:
            print(f'âŒ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}')
            rag_init_success = False
        
        if not rag_init_success:
            print('\nâŒ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.')
            return False
        
        print('\n' + '='*70)
        
        # í…ŒìŠ¤íŠ¸ 3: ì»¨í…ìŠ¤íŠ¸ ì—†ëŠ” ì§ì ‘ ì§ˆì˜
        print('ğŸ” í…ŒìŠ¤íŠ¸ 3: ì»¨í…ìŠ¤íŠ¸ ì—†ëŠ” ì§ì ‘ LLM ì§ˆì˜')
        print('-' * 50)
        
        direct_questions = [
            "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "Dockerì˜ ì£¼ìš” ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í—¥ì‚¬ê³ ë‚  ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        ]
        
        direct_test_success = True
        
        for i, question in enumerate(direct_questions, 1):
            print(f'\nì§ˆë¬¸ {i}: {question}')
            
            try:
                start_time = time.time()
                answer = await rag_service.query_without_context(question)
                duration = time.time() - start_time
                
                print(f'âœ… ì‘ë‹µ ì‹œê°„: {duration:.3f}ì´ˆ')
                print(f'ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(answer)} ë¬¸ì')
                print(f'ğŸ“„ ì‘ë‹µ (ì²˜ìŒ 150ì): {answer[:150]}...')
                
            except Exception as e:
                print(f'âŒ ì§ì ‘ ì§ˆì˜ ì‹¤íŒ¨: {e}')
                direct_test_success = False
                break
        
        print('\n' + '='*70)
        
        # í…ŒìŠ¤íŠ¸ 4: ë¬¸ì„œ ê¸°ë°˜ RAG ì§ˆì˜ (í˜„ì¬ ë²¡í„° ìŠ¤í† ì–´ ë‚´ìš© ì‚¬ìš©)
        print('ğŸ” í…ŒìŠ¤íŠ¸ 4: ë¬¸ì„œ ê¸°ë°˜ RAG ì§ˆì˜')
        print('-' * 50)
        
        rag_questions = [
            "ì‚¬ì´íŠ¸ ê´€ë¦¬ ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë¬¸ì„œ ì—…ë¡œë“œëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
            "Figure Backendì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "RAG ì‹œìŠ¤í…œì´ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
        ]
        
        rag_test_success = True
        
        for i, question in enumerate(rag_questions, 1):
            print(f'\nì§ˆë¬¸ {i}: {question}')
            
            try:
                start_time = time.time()
                result = await rag_service.query(question, include_sources=True)
                duration = time.time() - start_time
                
                print(f'âœ… ì‘ë‹µ ì‹œê°„: {duration:.3f}ì´ˆ')
                print(f'ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(result["answer"])} ë¬¸ì')
                print(f'ğŸ“š ì†ŒìŠ¤ ë¬¸ì„œ ìˆ˜: {len(result.get("sources", []))}')
                print(f'ğŸ“„ ë‹µë³€ (ì²˜ìŒ 200ì): {result["answer"][:200]}...')
                
                # ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
                if result.get("sources"):
                    print(f'ğŸ“‹ ì°¸ì¡° ì†ŒìŠ¤:')
                    for j, source in enumerate(result["sources"][:2], 1):  # ì²˜ìŒ 2ê°œë§Œ í‘œì‹œ
                        metadata = source.get("metadata", {})
                        print(f'   {j}. {metadata.get("title", "ì œëª© ì—†ìŒ")} - {metadata.get("doc_type", "unknown")}')
                
            except Exception as e:
                print(f'âŒ RAG ì§ˆì˜ ì‹¤íŒ¨: {e}')
                rag_test_success = False
                break
        
        print('\n' + '='*70)
        
        # í…ŒìŠ¤íŠ¸ 5: ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ìš”ì•½, ê°ì •ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ)
        print('ğŸ” í…ŒìŠ¤íŠ¸ 5: ê³ ê¸‰ LLM ê¸°ëŠ¥')
        print('-' * 50)
        
        test_document = """
        Figure BackendëŠ” RAG(Retrieval-Augmented Generation) ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ë¬¸ì„œ ê´€ë¦¬ ë° ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
        ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œëŠ” ë‹¤ì¤‘ í”„ë¡œë°”ì´ë” ì§€ì›(Claude, Gemini, OpenAI), í—¥ì‚¬ê³ ë‚  ì•„í‚¤í…ì²˜ ì ìš©, 
        ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰, ì‹¤ì‹œê°„ ë¬¸ì„œ ì²˜ë¦¬ ë“±ì´ ìˆìŠµë‹ˆë‹¤. 
        ì´ ì‹œìŠ¤í…œì„ í†µí•´ ì‚¬ìš©ìëŠ” ë³µì¡í•œ ë¬¸ì„œë“¤ ì†ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë©°,
        ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ë©´ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        
        advanced_test_success = True
        
        # ë¬¸ì„œ ìš”ì•½ í…ŒìŠ¤íŠ¸
        try:
            print('\nğŸ“ ë¬¸ì„œ ìš”ì•½ í…ŒìŠ¤íŠ¸:')
            summary = await rag_service.summarize_document(test_document, max_length=100)
            print(f'âœ… ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)')
            print(f'ğŸ“„ ìš”ì•½: {summary}')
            
        except Exception as e:
            print(f'âŒ ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {e}')
            advanced_test_success = False
        
        # ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸
        try:
            print('\nğŸ˜Š ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸:')
            sentiment = await rag_service.analyze_sentiment(test_document)
            print(f'âœ… ê°ì • ë¶„ì„ ì™„ë£Œ')
            print(f'ğŸ“Š ê°ì •: {sentiment.get("sentiment", "Unknown")}')
            print(f'ğŸ¯ ì‹ ë¢°ë„: {sentiment.get("confidence", 0):.2%}')
            
        except Exception as e:
            print(f'âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}')
            advanced_test_success = False
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        try:
            print('\nğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:')
            keywords = await rag_service.extract_keywords(test_document, count=5)
            print(f'âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ')
            print(f'ğŸ·ï¸  í‚¤ì›Œë“œ: {", ".join(keywords[:5])}')
            
        except Exception as e:
            print(f'âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}')
            advanced_test_success = False
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print('\n' + 'ğŸ‰ í†µí•© RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼')
        print('=' * 70)
        
        tests = [
            ('ê°œë³„ ì»´í¬ë„ŒíŠ¸', embedding_test_success and llm_test_success),
            ('RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”', rag_init_success),
            ('ì§ì ‘ LLM ì§ˆì˜', direct_test_success),
            ('ë¬¸ì„œ ê¸°ë°˜ RAG ì§ˆì˜', rag_test_success),
            ('ê³ ê¸‰ LLM ê¸°ëŠ¥', advanced_test_success)
        ]
        
        success_count = sum(1 for _, success in tests if success)
        total_tests = len(tests)
        
        print(f'ğŸ“Š ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {success_count}/{total_tests}')
        print(f'ğŸ“ˆ ì„±ê³µë¥ : {success_count/total_tests*100:.1f}%')
        
        for test_name, success in tests:
            status = 'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'
            print(f'   {test_name}: {status}')
        
        overall_success = success_count >= 4  # ìµœì†Œ 4ê°œ í…ŒìŠ¤íŠ¸ ì„±ê³µ
        
        if overall_success:
            print('\nğŸ‰ í†µí•© RAG ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!')
            print('ğŸ’¡ Claude LLM + Gemini Embedding ì¡°í•©ì´ ì™„ë²½í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.')
            print('ğŸš€ ì´ì œ í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!')
        else:
            print('\nâŒ ì¼ë¶€ ê¸°ëŠ¥ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
            print('ğŸ”§ API í‚¤ ì„¤ì •ì´ë‚˜ ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.')
        
        # ìƒì„¸ í†µê³„ ì €ì¥
        test_results = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'configuration': {
                'llm_provider': settings.llm_provider,
                'embedding_provider': settings.embedding_provider,
                'llm_model': getattr(settings, f"{settings.llm_provider}_model", "Unknown"),
                'embedding_model': getattr(settings, f"{settings.embedding_provider}_embedding_model", "Unknown")
            },
            'test_results': {test_name: success for test_name, success in tests},
            'success_rate': success_count/total_tests*100,
            'overall_success': overall_success
        }
        
        # ê²°ê³¼ ì €ì¥
        with open('/app/integrated_rag_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        print(f'\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ /app/integrated_rag_test_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.')
        
        return overall_success
        
    except Exception as e:
        print(f'\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}')
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_integrated_rag_system())
    print(f'\nğŸ“Š ìµœì¢… ê²°ê³¼: {"SUCCESS" if success else "FAILED"}')
    exit(0 if success else 1) 